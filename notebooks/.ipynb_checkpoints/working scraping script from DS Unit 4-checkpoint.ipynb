{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indeed Scraper.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYpDckKZnPP7",
        "colab_type": "text"
      },
      "source": [
        "This was an optional objective during DS Unit 4 Sprint 2 Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pix9SU2VnKE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# for webscrapping\n",
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqnk1J4mnZoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pages = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "with open('C:/Users/Lenovo/DS-Unit-4-Sprint-1-NLP/module2-vector-representations/data_scientist_jobs.csv', 'a', encoding='utf-8', newline='') as f_output:\n",
        "    csv_print = csv.writer(f_output)\n",
        "    \n",
        "    file_is_empty = os.stat('C:/Users/Lenovo/DS-Unit-4-Sprint-1-NLP/module2-vector-representations/data_scientist_jobs.csv').st_size == 0\n",
        "    if file_is_empty:\n",
        "        csv_print.writerow(['Job Title', 'Company', 'Location', 'Summary'])\n",
        "        \n",
        "    # get data from url\n",
        "    for page in pages:\n",
        "        source = requests.get('https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco%2C+CA&fromage=15&radius=25&start={}'.format(page)).text\n",
        "\n",
        "        soup = BeautifulSoup(source, 'html.parser')\n",
        "        # print(soup.prettify())\n",
        "\n",
        "        for jobs in soup.find_all(class_='result'):\n",
        "        #     print(jobs.prettify())\n",
        "        #     print('------------')\n",
        "\n",
        "            try:\n",
        "                job_title = jobs.find('div', class_='title').text.strip()\n",
        "            except Exception as e:\n",
        "                job_title = None\n",
        "#             print('Job Title', job_title)\n",
        "\n",
        "            try:\n",
        "                company = jobs.span.text.strip()\n",
        "            except Exception as e:\n",
        "                company = None\n",
        "#             print('Company', company)\n",
        "    \n",
        "            try:\n",
        "                location = jobs.find('div', class_='location accessible-contrast-color-location').text.strip()\n",
        "            except Exception as e:\n",
        "                location = None\n",
        "#             print('Location', location)\n",
        "    \n",
        "            try:\n",
        "                summary = jobs.find('div', class_='summary').text.strip()\n",
        "            except Exception as e:\n",
        "                summary = None\n",
        "#             print('Summary', summary)\n",
        "            \n",
        "            csv_print.writerow([job_title, company, location, summary])\n",
        "\n",
        "\n",
        "# jobs = soup.find(class_='result')\n",
        "# print(jobs.prettify())\n",
        "\n",
        "# job_title = jobs.a.text.strip()\n",
        "# print(job_title)\n",
        "\n",
        "# company = jobs.span.text.strip()\n",
        "# print(company)\n",
        "\n",
        "# location = jobs.find('div', class_='location accessible-contrast-color-location').text.strip()\n",
        "# print(location)\n",
        "\n",
        "# summary = jobs.find('div', class_='summary').text.strip()\n",
        "# print(summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WA0_w74oODB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_scrapping = pd.read_csv('data_scientist_jobs.csv')\n",
        "df_scrapping.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}